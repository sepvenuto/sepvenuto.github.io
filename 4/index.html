<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4: Neural Radiance Field</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Project 4: Neural Radiance Field</h1>
            <p class="subtitle">CS180 • Neural Radiance Field (NeRF)</p>
        </header>
        
        <div style="padding: 20px;">
            <a href="../index.html">← Back to Portfolio</a>
        </div>

        <main>
            <!-- Overview Section -->
            <div class="project-content">
                <h2>Overview</h2>
                <div class="project-content-body">
                    <p>This project explores Neural Radiance Fields (NeRF), a technique for synthesizing novel views of complex 3D scenes from a set of 2D images. The project is divided into three main parts: camera calibration and 3D scanning using ArUco markers, fitting a neural field to 2D images, and finally training a full 3D NeRF from multi-view images. Through this project, I learned how to implement volume rendering, positional encoding, and the complete NeRF pipeline from scratch using PyTorch.</p>
                </div>
            </div>

            <!-- Part 0: Camera Calibration -->
<div class="project-content">
    <h2>Part 0: Camera Calibration and 3D Scanning</h2>
    <div class="project-content-body">
        
        <h3>Part 0.1: Camera Calibration</h3>
        <p>I calibrated my camera using ArUco markers to obtain the camera intrinsic matrix and distortion coefficients. I captured [NUMBER] calibration images from various angles and distances to ensure robust calibration.</p>
        
        <p><strong>Calibration Process:</strong></p>
        <ul>
            <li>Captured calibration images with varying angles and distances</li>
            <li>Detected ArUco tags using OpenCV's detector</li>
            <li>Extracted corner coordinates and corresponding 3D world points</li>
            <li>Used cv2.calibrateCamera() to compute intrinsic parameters</li>
        </ul>

        <p><strong>Calibration Results:</strong></p>

        <h3>Part 0.2: Capturing My Object</h3>
        
        <p>I chose to scan my bunnies jewlery holder. I placed it next to an ArUco marker and captured [NUMBER] images from different angles, maintaining consistent camera settings and zoom level as used during calibration.</p>

        <h3>Part 0.3: Camera Pose Estimation</h3>
        <p>Using the calibrated camera parameters, I estimated the camera pose for each captured image using cv2.solvePnP(). This provided the rotation and translation vectors that describe the camera's position and orientation relative to the ArUco tag.</p>

        <h4>Viser Visualization - Camera Frustums</h4>
        <p>Below are screenshots of the camera frustums visualized in 3D using Viser, showing the estimated poses for all captured images of the bunny scene.</p>
        
        <div class="image-gallery">
            <div class="image-container" style="max-width: 500px; margin: 0 auto;">
                <img src="./part0/bunnies_poses.png" alt="Viser visualization of camera poses">
                <div class="image-caption">Camera Frustums Showing All Capture Positions</div>
            </div>
            <div class="image-container" style="max-width: 500px; margin: 0 auto;">
                <img src="./part0/bunnies_poses_again.png" alt="Viser visualization alternative view">
                <div class="image-caption">Camera Frustums - Alternative Viewpoint</div>
            </div>
        </div>

        <p>The visualization shows the spatial distribution of camera positions around the object. Each camera frustum represents one captured image, with the frustum orientation showing the camera's viewing direction and the embedded image showing what was captured from that viewpoint.</p>
    </div>
</div>

            <!-- Part 1: 2D Neural Field -->
<div class="project-content">
    <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
    <div class="project-content-body">
        
        <h3>Implementation</h3>
        <h4>Network Architecture</h4>
        <p><strong>Model Details:</strong></p>
        <ul>
            <li>Number of layers: 4 hidden layers</li>
            <li>Layer width (channels): 360</li>
            <li>Positional encoding max frequency (L): 10</li>
            <li>Learning rate: 1e-2</li>
            <li>Optimizer: Adam</li>
            <li>Loss function: MSE</li>
            <li>Training iterations: 3000 epochs</li>
            <li>Batch size: 10,000 pixels</li>
        </ul>

        <p><strong>Architecture Description:</strong> I implemented an MLP with positional encoding that takes 2D pixel coordinates and outputs RGB colors. The positional encoding expands the 2D input using sinusoidal functions at multiple frequencies (L=10), creating a 42-dimensional vector. The network uses ReLU activations between linear layers and a Sigmoid activation at the output to constrain colors to [0,1].</p>

        <h3>Training Results - Provided Fox Image</h3>
        <p>Training progression on the provided fox image:</p>
        
        <div class="image-gallery">
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox.jpg" alt="Original fox image">
                <div class="image-caption">Original</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_0_frequency_10.png" alt="Fox epoch 0">
                <div class="image-caption">0 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_200_frequency_10.png" alt="Fox epoch 200">
                <div class="image-caption">200 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_400_frequency_10.png" alt="Fox epoch 400">
                <div class="image-caption">400 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_600_frequency_10.png" alt="Fox epoch 600">
                <div class="image-caption">600 iterations</div>
            </div>
        </div>

        <div class="image-gallery">
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_800_frequency_10.png" alt="Fox epoch 800">
                <div class="image-caption">800 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_1000_frequency_10.png" alt="Fox epoch 1000">
                <div class="image-caption">1000 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_1200_frequency_10.png" alt="Fox epoch 1200">
                <div class="image-caption">1200 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_1400_frequency_10.png" alt="Fox epoch 1400">
                <div class="image-caption">1400 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_1600_frequency_10.png" alt="Fox epoch 1600">
                <div class="image-caption">1600 iterations</div>
            </div>
        </div>

        <div class="image-gallery">
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_1800_frequency_10.png" alt="Fox epoch 1800">
                <div class="image-caption">1800 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_2000_frequency_10.png" alt="Fox epoch 2000">
                <div class="image-caption">2000 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_2200_frequency_10.png" alt="Fox epoch 2200">
                <div class="image-caption">2200 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_2400_frequency_10.png" alt="Fox epoch 2400">
                <div class="image-caption">2400 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_2600_frequency_10.png" alt="Fox epoch 2600">
                <div class="image-caption">2600 iterations</div>
            </div>
        </div>

        <div class="image-gallery">
            <div class="image-container" style="max-width: 200px; margin: 0 auto;">
                <img src="./part1/fox_rendered_epoch_2800_frequency_10.png" alt="Fox epoch 2800">
                <div class="image-caption">2800 iterations</div>
            </div>
            <div class="image-container" style="max-width: 200px; margin: 0 auto;">
                <img src="./part1/fox_L_10_width_360_final.png" alt="Fox final">
                <div class="image-caption">Final (3000 iters)</div>
            </div>
        </div>

        <h3>Training Results - My Cat Ashie</h3>
        <p>Training progression on my own image of my cat:</p>
        
        <div class="image-gallery">
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie.jpeg" alt="Original Ashie image">
                <div class="image-caption">Original</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_0_frequency_10.png" alt="Ashie epoch 0">
                <div class="image-caption">0 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_200_frequency_10.png" alt="Ashie epoch 200">
                <div class="image-caption">200 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_400_frequency_10.png" alt="Ashie epoch 400">
                <div class="image-caption">400 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_600_frequency_10.png" alt="Ashie epoch 600">
                <div class="image-caption">600 iterations</div>
            </div>
        </div>

        <div class="image-gallery">
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_800_frequency_10.png" alt="Ashie epoch 800">
                <div class="image-caption">800 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_1000_frequency_10.png" alt="Ashie epoch 1000">
                <div class="image-caption">1000 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_1200_frequency_10.png" alt="Ashie epoch 1200">
                <div class="image-caption">1200 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_1400_frequency_10.png" alt="Ashie epoch 1400">
                <div class="image-caption">1400 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_1600_frequency_10.png" alt="Ashie epoch 1600">
                <div class="image-caption">1600 iterations</div>
            </div>
        </div>

        <div class="image-gallery">
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_1800_frequency_10.png" alt="Ashie epoch 1800">
                <div class="image-caption">1800 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_2000_frequency_10.png" alt="Ashie epoch 2000">
                <div class="image-caption">2000 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_2200_frequency_10.png" alt="Ashie epoch 2200">
                <div class="image-caption">2200 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_2400_frequency_10.png" alt="Ashie epoch 2400">
                <div class="image-caption">2400 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_2600_frequency_10.png" alt="Ashie epoch 2600">
                <div class="image-caption">2600 iterations</div>
            </div>
        </div>

        <div class="image-gallery">
            <div class="image-container" style="max-width: 200px; margin: 0 auto;">
                <img src="./part1/ashie_rendered_epoch_2800_frequency_10.png" alt="Ashie epoch 2800">
                <div class="image-caption">2800 iterations</div>
            </div>
            <div class="image-container" style="max-width: 200px; margin: 0 auto;">
                <img src="./part1/ashie_L_10_width_360_final.png" alt="Ashie final">
                <div class="image-caption">Final (3000 iters)</div>
            </div>
        </div>

        <h3>Hyperparameter Exploration</h3>
        <p>I experimented with different positional encoding frequencies (L) and network widths to understand their impact on reconstruction quality.</p>

        <h4>Fox - Varying L and Width (2×2 Grid)</h4>
        <div class="image-gallery">
            <div class="image-container" style="max-width: 250px; margin: 0 auto;">
                <img src="./part1/fox_L_2_width_10_final.png" alt="Fox L=2, width=10">
                <div class="image-caption">L = 2, Width = 10</div>
            </div>
            <div class="image-container" style="max-width: 250px; margin: 0 auto;">
                <img src="./part1/fox_L_2_width_256_final.png" alt="Fox L=2, width=256">
                <div class="image-caption">L = 2, Width = 256</div>
            </div>
        </div>
        <div class="image-gallery">
            <div class="image-container" style="max-width: 250px; margin: 0 auto;">
                <img src="./part1/fox_L_10_width_10_final.png" alt="Fox L=10, width=10">
                <div class="image-caption">L = 10, Width = 10</div>
            </div>
            <div class="image-container" style="max-width: 250px; margin: 0 auto;">
                <img src="./part1/fox_L_10_width_256_final.png" alt="Fox L=10, width=256">
                <div class="image-caption">L = 10, Width = 256</div>
            </div>
        </div>

        <h4>Ashie - Varying L and Width (2×2 Grid)</h4>
        <div class="image-gallery">
            <div class="image-container" style="max-width: 250px; margin: 0 auto;">
                <img src="./part1/ashie_L_2_width_10_final.png" alt="Ashie L=2, width=10">
                <div class="image-caption">L = 2, Width = 10</div>
            </div>
            <div class="image-container" style="max-width: 250px; margin: 0 auto;">
                <img src="./part1/ashie_L_2_width_256_final.png" alt="Ashie L=2, width=256">
                <div class="image-caption">L = 2, Width = 256</div>
            </div>
        </div>
        <div class="image-gallery">
            <div class="image-container" style="max-width: 250px; margin: 0 auto;">
                <img src="./part1/ashie_L_10_width_10_final.png" alt="Ashie L=10, width=10">
                <div class="image-caption">L = 10, Width = 10</div>
            </div>
            <div class="image-container" style="max-width: 250px; margin: 0 auto;">
                <img src="./part1/ashie_L_10_width_256_final.png" alt="Ashie L=10, width=256">
                <div class="image-caption">L = 10, Width = 256</div>
            </div>
        </div>

        <p><strong>Observations:</strong> With very low values (L=2, width=10), the network struggles to capture fine details and produces blurry results. Increasing the positional encoding frequency (L) allows the network to represent higher-frequency details. Increasing the network width provides more capacity to learn complex patterns. The best results come from combining high frequency encoding (L=10) with sufficient network capacity (width=256 or 360).</p>

        <h3>PSNR Curves</h3>
        <div class="image-gallery">
            <div class="image-container" style="max-width: 450px; margin: 0 auto;">
                <img src="./part1/fox_psnr.png" alt="Fox PSNR curve">
                <div class="image-caption">Fox - PSNR vs Training Iterations</div>
            </div>
            <div class="image-container" style="max-width: 450px; margin: 0 auto;">
                <img src="./part1/ashie_psnr.png" alt="Ashie PSNR curve">
                <div class="image-caption">Ashie - PSNR vs Training Iterations</div>
            </div>
        </div>
    </div>
</div>
            <!-- Part 2: 3D NeRF -->
            <div class="project-content">
                <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
                <div class="project-content-body">
                    
                    <h3>Implementation Overview</h3>
                    <p>I implemented a complete NeRF pipeline including:</p>
                    <ul>
                        <li><strong>Camera-to-world coordinate transformation:</strong> Converting between camera and world coordinate systems</li>
                        <li><strong>Pixel-to-camera coordinate conversion:</strong> Using camera intrinsics to map pixels to 3D rays</li>
                        <li><strong>Ray generation:</strong> Creating rays with origins and directions for each pixel</li>
                        <li><strong>Point sampling along rays:</strong> Discretizing rays with perturbed sampling for regularization</li>
                        <li><strong>Neural radiance field network:</strong> MLP with positional encoding for predicting color and density</li>
                        <li><strong>Volume rendering:</strong> Integrating along rays to render final pixel colors</li>
                    </ul>

                    <h3>Part 2.1-2.3: Ray Generation and Sampling</h3>

                    <h4>Part 2.1: Create Rays from Cameras</h4>
                    <h5>Camera to World Coordinate Conversion</h5>
                    <p>I first expand the camera coordinates to include the homogeneous 1 coordinate. Then, I multiply the camera_to_world matrix by the camera coordinates, to obtain the world coordinates.</p>
                    <h5>Pixel to Camera Coordinate Conversion</h5>
                    <p>To enable batching, I do something similar to what we did in Project 3, where instead of doing matrix multiplication, I extract the values from K and then use broadcasting to create the equations. The x coordinate in camera is the (u_pixel - offset_x) * s / x_focal_length, and the y coordinate in camera is the (v_pixel - offset_y) * s / y_focal_length. The Z (depth) is the s, because s is the depth.</p>
                    <h5>Pixel to Ray Coordinate Conversion</h5>
                    <p>I first extract r_o=t from the K matrix, which is the 1st three rows of the last column of K. Then, I use my pixel_to_camera function on K, uv, and s=1. Then I use my camera_to_wolrd function on the resulting x_c with the c2w. I now have world coordinates. To calculate r_d, or the ray direction, I normalize the difference between my world point and the original ray r_o by dividing (X_w-r_o) by the L2 norm of (X_w-r_o). I then return r_o and r_d.</p>

                    <h4>Part 2.2: Sampling</h4>
                     <h5>Sampling Rays from Images</h5>
                    <p>I randomly generate N indicies of rays from my total number of rays. I then get the sampled pixels, rays_o, and rays_d from those indices. I precomputed the rays to save the time of re-computing them each time, and so that I could use batching and take advantage of the batch process.</p>
                    <h5>Sampling Points along rays</h5>
                    <p>I am using the MPS for this part to make it go quicker, so I first move to the MPS. Then, I set t = np.linspace(near, far, n_samples) to uniformly sample along the ray. Near=2.0 and Far = 6.0. Then, if I am meant to perturb the data for training purposes, I will randomly add in a "perturbation term" to the t value, so that the samples are randomly spaced instead of uniform. I then expand the t so that its shape matches that of r_o, before following the provided equation of point = r_o + r_d*t, which samples the points along the ray from starting at r_o and moving each interval of size t in direction r_d. I set num samples to 32.</p>

                    <h4>Putting the Dataloading All Together</h4>
                    <p>I created a RaysData class that takes in the Dataset to follow the format in the provided testing code. I first convert all images, K, and c2ws to floats and torch tensor types. Then, I create a meshgrid to create the pairings of possible (u, v) points, being sure to add the 0.5 per the instructions. I assign the self.images, self.K, self.c2ws, self.H, self.W to the RaysData. I then calculate the total rays per image, total rays, and flatten all of the pixels across all images. These will helpful for the ray sampling. Then, for each image (therefore each camera location, and so then each c2w), I use my pixel_to_ray function to calculate the rays_o and rays_d for each image. This way I can take advantage of batching and not need to recalculate with different c2ws later when I am sampling from my dataset.</p>
                    
                    <h4>Visualization of Rays and Samples</h4>                    
                    <div class="image-gallery">
                        <div class="image-container" style="max-width: 600px; margin: 0 auto;">
                            <img src="" alt="Point cloud visualization">
                            <div class="image-caption">Camera Frustums and Sample Points in 3D Space</div>
                        </div>
                    </div>

                    <p><strong>Sampling Parameters:</strong></p>
                    <ul>
                        <li>Near plane: 2.0 </li>
                        <li>Far plane: 6.0 </li>
                        <li>Number of samples per ray: 32</li>
                        <li>Perturbation applied: Yes</li>
                    </ul>

                    <h3>Part 2.4: Neural Radiance Field Network</h3>
                    
                    <p><strong>Network Architecture:</strong></p>
                    <ul>
                        <li>Input: 3D position (x, y, z) + 3D view direction (θ, φ)</li>
                        <li>Position encoding frequency: L = 10</li>
                        <li>Direction encoding frequency: L = 4</li>
                        <li>Number of Linear layers (position path): 8</li>
                        <li>Number of Linear layers (RGB path): 4 </li>
                        <li>Hidden layer width: 256</li>
                        <li>Skip connections at layer: 5 (concatenates x_PE)</li>
                        <li>Output: RGB color (3 channels) + density σ (1 channel)</li>
                    </ul>

                    <p>My network architecture follows the diagram provided in the instructions. Above, you can see the parameters that I chose for my model. For the model, I created different breaks between where we had to inject the input, which ended up being breaks between layers. My model has a fork where it outputs a value for the density along with the rgb value, which also takes in the injected x_PE. I also created a function to calculate the dimensions of the x and r_d after they were positionally encoded. </p>

                    <h3>Part 2.5: Volume Rendering</h3>
                    <p>I implemented the discrete volume rendering equation to composite colors and densities along rays into final pixel colors. The rendering equation accumulates contributions from each sample point, weighted by transmittance and opacity. To do this, we have the sigmas, which represent the density, deltas, which represent the step size/distance between each sample along our ray, and the opacity, which is the alpha value that we calculate. We can look at opacity as the probability that the given ray hits something and ends at the given point. T_i is the probability that we reach sample i without hitting anything first, or the transmittance. This is calculated by taking the product of the probabilities that we don't stop at all of the points before it, or (1-a_j) for all j less than i. We will then weight the color at each sample by the probability we hit that sample * the probability that it stops at that sample, or T_i * a_i. These can act as a "weight" on how much each sample will contrbute to the final color of what we view.</p>

                    <h3>Training Results - Lego Dataset</h3>
                    
                    <h4>Training Progression</h4>
        <p>Training progression showing how the NeRF learns to reconstruct the 3D scene over time:</p>
        
        <div class="image-gallery">
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part2/nerf_epoch_0000.png" alt="NeRF epoch 0">
                <div class="image-caption">0 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part2/nerf_epoch_0200.png" alt="NeRF epoch 200">
                <div class="image-caption">200 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part2/nerf_epoch_0400.png" alt="NeRF epoch 400">
                <div class="image-caption">400 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part2/nerf_epoch_0600.png" alt="NeRF epoch 600">
                <div class="image-caption">600 iterations</div>
            </div>
            <div class="image-container" style="max-width: 180px; margin: 0 auto;">
                <img src="./part2/nerf_epoch_0800.png" alt="NeRF epoch 800">
                <div class="image-caption">800 iterations</div>
            </div>
        </div>

        <div class="image-gallery">
            <div class="image-container" style="max-width: 200px; margin: 0 auto;">
                <img src="./part2/nerf_epoch_1000.png" alt="NeRF epoch 1000">
                <div class="image-caption">1000 iterations</div>
            </div>
            <div class="image-container" style="max-width: 200px; margin: 0 auto;">
                <img src="./part2/nerf_training_progress.png" alt="Training progress comparison">
                <div class="image-caption">Training Progress Summary</div>
            </div>
        </div>

        <h4>PSNR Curve - Validation Set</h4>
        <div class="image-gallery">
            <div class="image-container" style="max-width: 600px; margin: 0 auto;">
                <img src="./part2/nerf_psnr_curve.png" alt="PSNR validation curve">
                <div class="image-caption">PSNR on Validation Images</div>
            </div>
        </div>

                    <p><strong>Training Details:</strong></p>
                    <ul>
                        <li>Learning rate: 5e-4</li>
                        <li>Optimizer: Adam</li>
                        <li>Batch size: 10000 rays</li>
                        <li>Points per ray: 64</li>
                        <li>Total iterations: 1010</li>
                        <li>Final validation PSNR: 23.18 dB</li>
                    </ul>

                    <h4>Novel View Synthesis Video</h4>
                    <p>Spherical rendering of the Lego bulldozer from novel viewpoints using the test camera poses:</p>
                    
                    <!-- Alternative GIF version -->
                    <div class="image-gallery">
                        <div class="image-container" style="max-width: 600px; margin: 0 auto;">
                            <img src="./part2/lego_spherical.gif" alt="Novel views GIF">
                            <div class="image-caption">Novel View Synthesis - Lego Dataset (GIF)</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Part 2.6: Own Data -->
            <div class="project-content">
                <h2>Part 2.6: Training NeRF on My Own Data</h2>
                <div class="project-content-body">
                    
                    <h3>Dataset and Training Setup</h3>
                    <p><strong>Object:</strong> [Describe your object]</p>
                    
                    <p><strong>Dataset Statistics:</strong></p>
                    <ul>
                        <li>Training images: [NUMBER]</li>
                        <li>Validation images: [NUMBER]</li>
                        <li>Image resolution: [WIDTH] × [HEIGHT]</li>
                        <li>Focal length: [VALUE]</li>
                    </ul>

                    <p><strong>Hyperparameter Changes:</strong></p>
                    <ul>
                        <li>Near plane: [VALUE] (adjusted from 2.0)</li>
                        <li>Far plane: [VALUE] (adjusted from 6.0)</li>
                        <li>Samples per ray: [NUMBER]</li>
                        <li>[Any other changes you made and why]</li>
                    </ul>

                    <h3>Training Progression</h3>
                    <p>Intermediate renders showing the NeRF learning process:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container" style="max-width: 220px; margin: 0 auto;">
                            <img src="./part2/own_iter100.png" alt="Own data 100 iterations">
                            <div class="image-caption">100 iterations</div>
                        </div>
                        <div class="image-container" style="max-width: 220px; margin: 0 auto;">
                            <img src="./part2/own_iter500.png" alt="Own data 500 iterations">
                            <div class="image-caption">500 iterations</div>
                        </div>
                        <div class="image-container" style="max-width: 220px; margin: 0 auto;">
                            <img src="./part2/own_iter1000.png" alt="Own data 1000 iterations">
                            <div class="image-caption">1000 iterations</div>
                        </div>
                        <div class="image-container" style="max-width: 220px; margin: 0 auto;">
                            <img src="./part2/own_final.png" alt="Own data final">
                            <div class="image-caption">Final result</div>
                        </div>
                    </div>

                    <h3>Training Loss Curve</h3>
                    <div class="image-gallery">
                        <div class="image-container" style="max-width: 600px; margin: 0 auto;">
                            <img src="./part2/own_loss_curve.png" alt="Training loss">
                            <div class="image-caption">Training Loss over Iterations</div>
                        </div>
                    </div>

                    <h3>Novel View Synthesis</h3>
                    <p>Rendered video of a camera circling my object:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container" style="max-width: 600px; margin: 0 auto;">
                            <video width="600" controls>
                                <source src="./part2/own_novel_views.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <div class="image-caption">Novel View Synthesis - My Object</div>
                        </div>
                    </div>

                    <!-- Alternative: GIF -->
                    <!-- <div class="image-gallery">
                        <div class="image-container" style="max-width: 600px; margin: 0 auto;">
                            <img src="./part2/own_novel_views.gif" alt="Own object novel views">
                            <div class="image-caption">Novel View Synthesis - My Object</div>
                        </div>
                    </div> -->

                    <h3>Discussion</h3>
                    <p>[Discuss any challenges you faced, what worked well, what didn't work as expected, and how the results compare to the Lego dataset]</p>
                </div>
            </div>

        


             
        </main>

        <footer>
            <p>&copy; 2025 CS180 Portfolio • Sylvie Venuto • UC Berkeley</p>
        </footer>
    </div>
</body>
</html>
