<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5A: The Power of Diffusion Models</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Project 5A: The Power of Diffusion Models</h1>
            <p class="subtitle">CS180 • Diffusion Models and Image Generation</p>
        </header>
        
        <div style="padding: 20px;">
            <a href="../index.html">← Back to Portfolio</a>
        </div>

        <main>

            <!-- Part 0: Setup -->
            <div class="project-content">
                <h2>Part 0: Setup and Text Prompts</h2>
                <div class="project-content-body">
                    
                    
                    <h3>Text Prompts and Embeddings</h3>
                    <p><strong>Random Seed Used:</strong> 100 </p>
                    <p>These are my prompts: You have the embeddings for the following prompts
['a high quality photo of a strong blonde woman with blue eyes',
 'a portrait of a college student with straight blonde hair',
 'a photo of a woman weightlifting at the gym',
 'a CrossFit athlete with blonde hair doing a workout',
 'a portrait of a brown-haired woman with green eyes and curly hair',
 'two sisters with different hair colors standing together',
 'a Berkeley student studying computer science',
 'a grey cat sitting peacefully',
 'a tabby cat lounging in sunlight',
 'two cats playing together',
 'a fluffy grey cat portrait',
 'a striped tabby cat looking at camera',
 'a CrossFit gym with equipment',
 'a woman doing a snatch in a CrossFit gym',
 'a Formula 1 race car on the track',
 'an F1 car speeding around a corner',
 'a Formula 1 paddock scene',
 'a woman playing video games',
 'an Overwatch character in action',
 'UC Berkeley campus with the Campanile tower',
 'Sather Tower at UC Berkeley',
 'a computer science classroom',
 'a student studying French in a library',
 'the Berkeley campus on a sunny day',
 'a cornflower blue gradient background',
 'an oil painting of a sunset',
 'a watercolor painting of flowers',
 'a lithograph of a waterfall',
 'an oil painting of an old man',
 'an oil painting of people around a campfire',
 'a rocket ship launching into space',
 'a lighthouse on a cliff',
 'a skull',
 'a waterfall in a forest',
 'a peaceful mountain landscape',
 'a busy city street at night',
 'a cat face close up',
 'a tiger in the jungle',
 'the Eiffel Tower',
 'the Golden Gate Bridge',
 'a coffee cup on a table',
 'a book open on a desk',
 'a Formula 1 car in cornflower blue',
 'a CrossFit gym with blue lighting',
 'a grey cat watching a computer screen',
 'a Berkeley student at a CrossFit competition',
 'two sisters watching Formula 1 together',
 'a tabby cat sleeping on a French textbook',
 'a high quality photo',
 'a professional esports tournament',
 'a cozy study space with cats',
 'a sunrise over San Francisco Bay',
 'a vintage Formula 1 poster',
 'a minimalist gym interior',
 'a French café in Paris',
 'a blonde woman studying at a library desk',
 'a blonde woman lifting weights at the gym',
 'a college student reading computer science textbooks',
 'a strong woman doing a deadlift',
 'a woman typing code on a laptop',
 'a CrossFit athlete doing pull-ups',
 'a student with blonde hair in a lecture hall',
 'a woman in workout clothes holding dumbbells',
 'a Berkeley student walking across campus',
 'a woman running on a track',
 'a blonde athlete stretching before a workout',
 'a focused student studying late at night',
 'a woman doing a handstand in a gym',
 'a college student drinking coffee while coding',
 'two sisters working out together',
 'two sisters studying together in a library',
 'a grey cat and a tabby cat cuddling',
 'a blonde woman and a brown-haired woman smiling',
 'a woman in a cornflower blue CrossFit shirt',
 'a Formula 1 driver in a racing suit',
 'an Overwatch player at a gaming setup',
 'a barbell loaded with weights',
 'a laptop showing lines of code',
 'a French language textbook open on a desk',
 'a gym filled with kettlebells and ropes',
 'a bookshelf full of computer science books',
 'a racing helmet on a pit wall',
 'a gaming keyboard with colorful lights',
 'a woman celebrating a personal record lift',
 'a student acing a computer science exam',
 'a peaceful meditation pose in a gym',
 'an intense study session with notes everywhere',
 'a sunrise workout at an outdoor gym',
 'a late night coding session with desk lamp',
 'a victory celebration at a CrossFit competition',
 'a successful project presentation in class',
 '']
                    I was partially inspired by common examples of ananagrams/hybrids I saw online so I knew I would have some to work with if my others don't work, and then I also included my interests of crossfit and video games, and my studying computer science and French, and going to Berkeley. I also have descriptions of myself and my sister, as well as our two pet cats</p>
                    
                    <h3>Sample Generated Images</h3>
                    <p>Below are images generated from my custom text prompts at different inference steps:</p>
                    
                    <h4>Prompt 1: "a Berkeley student studying computer science"</h4>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./0/a Berkeley student studying computer science with num_inference_steps=20 stage 1.png" alt="Berkeley student 20 steps stage 1">
                            <div class="image-caption">20 inference steps - Stage 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/a Berkeley student studying computer science with num_inference_steps=20 stage 2.png" alt="Berkeley student 20 steps stage 2">
                            <div class="image-caption">20 inference steps - Stage 2</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/a Berkeley student studying computer science with num_inference_steps=500 stage 1.png" alt="Berkeley student 500 steps stage 1">
                            <div class="image-caption">500 inference steps - Stage 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/a Berkeley student studying computer science with num_inference_steps=500 stage 2.png" alt="Berkeley student 500 steps stage 2">
                            <div class="image-caption">500 inference steps - Stage 2</div>
                        </div>
                    </div>

                    <h4>Prompt 2: "a grey cat watching a computer screen"</h4>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./0/a grey cat watching a computer screen with num_inference_steps=20 stage 1.png" alt="Grey cat 20 steps stage 1">
                            <div class="image-caption">20 inference steps - Stage 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/a grey cat watching a computer screen with num_inference_steps=20 stage 2.png" alt="Grey cat 20 steps stage 2">
                            <div class="image-caption">20 inference steps - Stage 2</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/a grey cat watching a computer screen with num_inference_steps=500 stage 1.png" alt="Grey cat 500 steps stage 1">
                            <div class="image-caption">500 inference steps - Stage 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/a grey cat watching a computer screen with num_inference_steps=500 stage 2.png" alt="Grey cat 500 steps stage 2">
                            <div class="image-caption">500 inference steps - Stage 2</div>
                        </div>
                    </div>

                    <h4>Prompt 3: "two sisters watching Formula 1 together"</h4>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./0/two sisters watching Formula 1 together with num_inference_steps=20 stage 1.png" alt="Sisters F1 20 steps stage 1">
                            <div class="image-caption">20 inference steps - Stage 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/two sisters watching Formula 1 together with num_inference_steps=20 stage 2.png" alt="Sisters F1 20 steps stage 2">
                            <div class="image-caption">20 inference steps - Stage 2</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/two sisters watching Formula 1 together with num_inference_steps=500 stage 1.png" alt="Sisters F1 500 steps stage 1">
                            <div class="image-caption">500 inference steps - Stage 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./0/two sisters watching Formula 1 together with num_inference_steps=500 stage 2.png" alt="Sisters F1 500 steps stage 2">
                            <div class="image-caption">500 inference steps - Stage 2</div>
                        </div>
                    </div>

                    <h4>Observations</h4>
                    <p>The more specific or real-world based (berkeley, formula 1) the prompt, the less likeley the model was to be able to adhear to it. For the computer science student at Berkeley, I thought it was interesting that there was nothing suggesting computer science in the images. It was interesting that as we increase the number of inference steps, the details of the person became more realistic (the refelction on the glasses, for example). Also, the B for berkeley appeared on the shirt. For the formula one, as we increased the number of inference steps, we also increased the amountof detail in the women, but it was interesting that the formula one never really appeared much beyond the one woman's shirt rsembling the VCARB livery in the less steps. It is also interesting that the glasses became sort of neon and the colors more vibrant with more steps. For the cat, the alignment with the prompt was pretty good with less inference steps, although the computer screen was blank. As we increased the steps, the image got more detailed, but the screenhad details that don't make as much sense, and the colors seemed oversaturated. Also, the cat is no longer looking at the computer.</p>
                </div>
            </div>

            <!-- Part 1: Sampling Loops -->
            <div class="project-content">
                <h2>Part 1: Sampling Loops</h2>
                <div class="project-content-body">
                    
                    <h3>1.1 Implementing the Forward Process</h3>
                    <pre><code>def forward(im, t):
  """
  Args:
    im : torch tensor of size (1, 3, 64, 64) representing the clean image
    t : integer timestep

  Returns:
    im_noisy : torch tensor of size (1, 3, 64, 64) representing the noisy image at timestep t
  """
  with torch.no_grad():
    # ===== your code here! ====
    a_t = alphas_cumprod[t]
    epsilon = torch.randn_like(im)
    sqrt_a_t = torch.sqrt(a_t)
    sqrt_1_minus_a_t = torch.sqrt(1 - a_t)
    im_noisy = sqrt_a_t * im + sqrt_1_minus_a_t * epsilon

    # ===== end of code ====
  return im_noisy
                    </code></pre>
                    <h4>Noisy Campanile Images</h4>
                    <p>Berkeley Campanile with noise added at different timesteps. As t increases, more noise is added:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.1/noise level_ 250.png" alt="Noisy at t=250">
                            <div class="image-caption">t=250</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.1/noise level_ 500.png" alt="Noisy at t=500">
                            <div class="image-caption">t=500</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.1/noise level_ 750.png" alt="Noisy at t=750">
                            <div class="image-caption">t=750</div>
                        </div>
                    </div>

                    <h3>1.2 Classical Denoising</h3>
                    <p>Attempting to denoise using Gaussian blur filtering. As we can see, classical methods struggle to effectively denoise these images:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.2/noisy image level 250.png" alt="Noisy t=250">
                            <div class="image-caption">Noisy Image at t=250</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.2/denoised image level 250.png" alt="Gaussian denoised t=250">
                            <div class="image-caption">Gaussian Blur Denoising at t=250</div>
                        </div>
                    </div>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.2/noisy image level 500.png" alt="Noisy t=500">
                            <div class="image-caption">Noisy Image at t=500</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.2/denoised image level 500.png" alt="Gaussian denoised t=500">
                            <div class="image-caption">Gaussian Blur Denoising at t=500</div>
                        </div>
                    </div>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.2/noisy image level 750.png" alt="Noisy t=750">
                            <div class="image-caption">Noisy Image at t=750</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.2/denoised image level 750.png" alt="Gaussian denoised t=750">
                            <div class="image-caption">Gaussian Blur Denoising at t=750</div>
                        </div>
                    </div>


                    <h3>1.3 One-Step Denoising</h3>
                    <p>Using the pretrained diffusion model UNet for one-step denoising with the prompt "a high quality photo". For each noise level, we show the noisy image and the one-step denoised result:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.3/im_noisy%20campanilie%20with%20t%3D250.png" alt="Noisy t=250">
                            <div class="image-caption">Noisy Campanile at t=250</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.3/Denoised%20Campanilie%20with%20t%3D250.png" alt="One-step denoised t=250">
                            <div class="image-caption">One-Step Denoised at t=250</div>
                        </div>
                    </div>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.3/im_noisy%20campanilie%20with%20t%3D500.png" alt="Noisy t=500">
                            <div class="image-caption">Noisy Campanile at t=500</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.3/Denoised%20Campanilie%20with%20t%3D500.png" alt="One-step denoised t=500">
                            <div class="image-caption">One-Step Denoised at t=500</div>
                        </div>
                    </div>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.3/im_noisy%20campanilie%20with%20t%3D750.png" alt="Noisy t=750">
                            <div class="image-caption">Noisy Campanile at t=750</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.3/Denoised%20Campanilie%20with%20t%3D750.png" alt="One-step denoised t=750">
                            <div class="image-caption">One-Step Denoised at t=750</div>
                        </div>
                    </div>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.3/orginal%20Campanilie.png" alt="Original Campanile">
                            <div class="image-caption">Original Campanile (for reference)</div>
                        </div>
                    </div>

                    <h3>1.4 Iterative Denoising</h3>
                    <p><strong>Parameters:</strong></p>
                    <ul>
                        <li>Starting index (i_start): 10</li>
                        <li>Stride: 30</li>
                        <li>Timestep range: [990, 960, ..., 0]</li>
                    </ul>

                    <h4>Denoising Progression</h4>
                    <p>Showing the iterative denoising process starting from t=690. Each image shows progressively less noise as we move through the denoising loop (loop_num represents the iteration number):</p>
                    <pre><code>strided_timesteps = list(range(990, -1, -30))
                    
stage_1.scheduler.set_timesteps(timesteps=strided_timesteps)
                    </code></pre>
                    <pre><code>def iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps, display=True):
  image = im_noisy

  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      # Get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # get `alpha_cumprod` and `alpha_cumprod_prev` for timestep t from `alphas_cumprod`
      # compute `alpha`
      # compute `beta`
      # ===== your code here! =====
      alpha_t_bar = alphas_cumprod[t]
      alpha_t_prime_bar = alphas_cumprod[prev_t]
      alpha_t = alpha_t_bar / alpha_t_prime_bar
      beta_t = 1 - alpha_t
      # ==== end of code ====

      # Get noise estimate
      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds.half().cuda(),
          return_dict=False
      )[0]

      # Split estimate into noise and variance estimate
      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)

      # compute `pred_prev_image` (x_{t'}), the DDPM estimate for the image at the
      # next timestep, which is slightly less noisy. Use the equation 3.
      # This is the core of DDPM
      # ===== your code here! =====
      sqrt_alpha_t_bar = torch.sqrt(alpha_t_bar)
      sqrt_1_minus_alpha_t_bar = torch.sqrt(1 - alpha_t_bar)
      x_0 = (image - sqrt_1_minus_alpha_t_bar * noise_est) / sqrt_alpha_t_bar
      sqrt_alpha_t_prime_bar = torch.sqrt(alpha_t_prime_bar)
      sqrt_alpha_t = torch.sqrt(alpha_t)
      term1 = (sqrt_alpha_t_prime_bar * beta_t) / (1 - alpha_t_bar) * x_0
      term2 = (sqrt_alpha_t * (1 - alpha_t_prime_bar)) / (1 - alpha_t_bar) * image
      pred_prev_image = add_variance(predicted_variance, t, term1 + term2)

      # ==== end of code ====
      if i % 5 == 0:
        save_images = True
        save_image("A/1/1.4", img_name=f"predicted_img_t_{t}_loop_num_{i}", im = pred_prev_image)
        show_image(pred_prev_image, f"predicted_img_t_{t}_loop_num_{i}")
      image = pred_prev_image

    clean = image.cpu().detach().numpy()
    # if display:
    #   show_image(clean, f"clean with t={t}")
  return clean</code></pre>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.4/predicted_img_t_690_loop_num_10.png" alt="Loop 10">
                            <div class="image-caption">Loop 10 (t=690, most noisy)</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.4/predicted_img_t_540_loop_num_15.png" alt="Loop 15">
                            <div class="image-caption">Loop 15 (t=540)</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.4/predicted_img_t_390_loop_num_20.png" alt="Loop 20">
                            <div class="image-caption">Loop 20 (t=390)</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.4/predicted_img_t_240_loop_num_25.png" alt="Loop 25">
                            <div class="image-caption">Loop 25 (t=240)</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.4/predicted_img_t_90_loop_num_30.png" alt="Loop 30">
                            <div class="image-caption">Loop 30 (t=90, cleanest)</div>
                        </div>
                    </div>

                    <h4>Comparison: Iterative vs One-Step vs Gaussian</h4>
                    <p>Comparing all three denoising methods at t=690:</p>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.4/clean with t=690.png" alt="Iterative final">
                            <div class="image-caption">Iterative Denoising (Best Result)</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.4/clean_one_step with t=690.png" alt="One-step">
                            <div class="image-caption">One-Step Denoising</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.4/blur_filtered with t=690.png" alt="Gaussian">
                            <div class="image-caption">Gaussian Blur</div>
                        </div>
                    </div>

                    <h3>1.5 Diffusion Model Sampling</h3>
                    <p>Generating images from pure noise using the prompt "a high quality photo". These images are created by starting with random noise (t=1000) and iteratively denoising:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.5/generated_im_0.png" alt="Sample 1">
                            <div class="image-caption">Sample 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.5/generated_im_1.png" alt="Sample 2">
                            <div class="image-caption">Sample 2</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.5/generated_im_2.png" alt="Sample 3">
                            <div class="image-caption">Sample 3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.5/generated_im_3.png" alt="Sample 4">
                            <div class="image-caption">Sample 4</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.5/generated_im_4.png" alt="Sample 5">
                            <div class="image-caption">Sample 5</div>
                        </div>
                    </div>

                    <h3>1.6 Classifier-Free Guidance (CFG)</h3>
                    <p><strong>CFG Scale:</strong> γ = 7</p>
                    <p>Implementing Classifier-Free Guidance using the formula: ε = ε_u + γ(ε_c - ε_u), where ε_c is the conditional noise estimate and ε_u is the unconditional noise estimate. CFG greatly improves image quality at the expense of diversity.</p>
                                        
                    <pre><code>def iterative_denoise_cfg(im_noisy, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  image = im_noisy

  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      # Get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # Get `alpha_cumprod`, `alpha_cumprod_prev`, `alpha`, `beta`
      # ===== your code here! =====
      alpha_t_bar = alphas_cumprod[t]
      alpha_t_prime_bar = alphas_cumprod[prev_t]
      alpha_t = alpha_t_bar / alpha_t_prime_bar
      beta_t = 1 - alpha_t
      # ==== end of code ====

      # Get cond noise estimate
      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds.half().cuda(),
          return_dict=False
      )[0]

      # Get uncond noise estimate
      uncond_model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds.half().cuda(),
          return_dict=False
      )[0]

      # Split estimate into noise and variance estimate
      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)
      uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)

      # Compute the CFG noise estimate based on equation 4
      # ===== your code here! =====
      cfg_noise = uncond_noise_est + scale * (noise_est - uncond_noise_est)
      # ==== end of code ====


      # Get `pred_prev_image`, the next less noisy image.
      # ===== your code here! =====
      sqrt_alpha_t_bar = torch.sqrt(alpha_t_bar)
      sqrt_1_minus_alpha_t_bar = torch.sqrt(1 - alpha_t_bar)
      x_0 = (image - sqrt_1_minus_alpha_t_bar * cfg_noise) / sqrt_alpha_t_bar
      sqrt_alpha_t_prime_bar = torch.sqrt(alpha_t_prime_bar)
      sqrt_alpha_t = torch.sqrt(alpha_t)
      term1 = (sqrt_alpha_t_prime_bar * beta_t) / (1 - alpha_t_bar) * x_0
      term2 = (sqrt_alpha_t * (1 - alpha_t_prime_bar)) / (1 - alpha_t_bar) * image
      pred_prev_image = add_variance(predicted_variance, t, term1 + term2)
      # ==== end of code ====

      image = pred_prev_image

    clean = image.cpu().detach().numpy()

  return clean</code></pre>

                    <h4>Improved Samples with CFG</h4>
                    <p>These images are generated with CFG scale of 7, showing dramatically improved quality:</p>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.6/generated_im_0.png" alt="CFG Sample 1">
                            <div class="image-caption">CFG Sample 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.6/generated_im_1.png" alt="CFG Sample 2">
                            <div class="image-caption">CFG Sample 2</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.6/generated_im_2.png" alt="CFG Sample 3">
                            <div class="image-caption">CFG Sample 3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.6/generated_im_3.png" alt="CFG Sample 4">
                            <div class="image-caption">CFG Sample 4</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.6/generated_im_4.png" alt="CFG Sample 5">
                            <div class="image-caption">CFG Sample 5</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Part 1.7: Image-to-Image Translation -->
            <div class="project-content">
                <h3>SDEdit on Test Images</h3>
                    <p>Using SDEdit to project images onto the natural image manifold with the conditioning prompt "a high quality photo". The more noise we add (lower i_start values), the larger the edits become.</p>
                    
                    <h4>Campanile Edits</h4>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/denoised_campaniele_i_start_1.png" alt="i_start=1">
                            <div class="image-caption">i_start=1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/denoised_campaniele_i_start_3.png" alt="i_start=3">
                            <div class="image-caption">i_start=3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/denoised_campaniele_i_start_5.png" alt="i_start=5">
                            <div class="image-caption">i_start=5</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/denoised_campaniele_i_start_7.png" alt="i_start=7">
                            <div class="image-caption">i_start=7</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/denoised_campaniele_i_start_10.png" alt="i_start=10">
                            <div class="image-caption">i_start=10</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/denoised_campaniele_i_start_20.png" alt="i_start=20">
                            <div class="image-caption">i_start=20</div>
                        </div>
                    </div>

            <!-- Part 1.7.1: Hand-Drawn and Web Images -->
            <div class="project-content">
                <h2>Part 1.7.1: Editing Hand-Drawn and Web Images</h2>
                <div class="project-content-body">
                    
                    <h3>Web Image</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/1/web_im.png" alt="Web image original">
                            <div class="image-caption">Original Web Image</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/web_i_start1.png" alt="i_start=1">
                            <div class="image-caption">i_start=1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/web_i_start3.png" alt="i_start=3">
                            <div class="image-caption">i_start=3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/web_i_start5.png" alt="i_start=5">
                            <div class="image-caption">i_start=5</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/web_i_start7.png" alt="i_start=7">
                            <div class="image-caption">i_start=7</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/web_i_start10.png" alt="i_start=10">
                            <div class="image-caption">i_start=10</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/web_i_start20.png" alt="i_start=20">
                            <div class="image-caption">i_start=20</div>
                        </div>
                    </div>

                    <h3>Hand-Drawn Image 1: Castle</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_castle.png" alt="Drawing 1 original">
                            <div class="image-caption">Original Drawing: Castle</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_castle_start_1.png" alt="i_start=1">
                            <div class="image-caption">i_start=1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_castle_start_3.png" alt="i_start=3">
                            <div class="image-caption">i_start=3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_castle_start_5.png" alt="i_start=5">
                            <div class="image-caption">i_start=5</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_castle_start_7.png" alt="i_start=7">
                            <div class="image-caption">i_start=7</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_castle_start_10.png" alt="i_start=10">
                            <div class="image-caption">i_start=10</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_castle_start_20.png" alt="i_start=20">
                            <div class="image-caption">i_start=20</div>
                        </div>
                    </div>

                    <h3>Hand-Drawn Image 2: Tree</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_tree.png" alt="Drawing 2 original">
                            <div class="image-caption">Original Drawing: Tree</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_tree_start_1.png" alt="i_start=1">
                            <div class="image-caption">i_start=1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_tree_start_3.png" alt="i_start=3">
                            <div class="image-caption">i_start=3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_tree_start_5.png" alt="i_start=5">
                            <div class="image-caption">i_start=5</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_tree_start_7.png" alt="i_start=7">
                            <div class="image-caption">i_start=7</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_tree_start_10.png" alt="i_start=10">
                            <div class="image-caption">i_start=10</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/1/hand_drawn_tree_start_20.png" alt="i_start=20">
                            <div class="image-caption">i_start=20</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Part 1.7.2: Inpainting -->
            <div class="project-content">
                <h2>Part 1.7.2: Inpainting</h2>
                <div class="project-content-body">
                    <p>Implementing inpainting using the RePaint algorithm. At each denoising step, we replace the unmasked regions with the original image (with appropriate noise added), while allowing the model to generate content in the masked regions.</p>
                    <p><strong>Formula:</strong> x_t ← m·x_t + (1-m)·forward(x_orig, t)</p>
                    <pre><code>def inpaint(original_image, mask, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  image = torch.randn_like(original_image).to(device).half()
  original_image = original_image.to(device).half()
  mask = mask.to(device).half()
    # use your previous `iterative_denoise_cfg` function and make the appropriate changes
  with torch.no_grad():
    for i in range(0, len(timesteps) - 1):
      # Get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # Get `alpha_cumprod`, `alpha_cumprod_prev`, `alpha`, `beta`
      # ===== your code here! =====
      alpha_t_bar = alphas_cumprod[t]
      alpha_t_prime_bar = alphas_cumprod[prev_t]
      alpha_t = alpha_t_bar / alpha_t_prime_bar
      beta_t = 1 - alpha_t
      # ==== end of code ====

      # Get cond noise estimate
      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds.half().cuda(),
          return_dict=False
      )[0]

      # Get uncond noise estimate
      uncond_model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds.half().cuda(),
          return_dict=False
      )[0]

      # Split estimate into noise and variance estimate
      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)
      uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)

      # Compute the CFG noise estimate based on equation 4
      # ===== your code here! =====
      cfg_noise = uncond_noise_est + scale * (noise_est - uncond_noise_est)
      # ==== end of code ====


      # Get `pred_prev_image`, the next less noisy image.
      # ===== your code here! =====
      sqrt_alpha_t_bar = torch.sqrt(alpha_t_bar)
      sqrt_1_minus_alpha_t_bar = torch.sqrt(1 - alpha_t_bar)
      x_0 = (image - sqrt_1_minus_alpha_t_bar * cfg_noise) / sqrt_alpha_t_bar
      sqrt_alpha_t_prime_bar = torch.sqrt(alpha_t_prime_bar)
      sqrt_alpha_t = torch.sqrt(alpha_t)
      term1 = (sqrt_alpha_t_prime_bar * beta_t) / (1 - alpha_t_bar) * x_0
      term2 = (sqrt_alpha_t * (1 - alpha_t_prime_bar)) / (1 - alpha_t_bar) * image
      pred_prev_image = add_variance(predicted_variance, t, term1 + term2)
      # ==== end of code ====
      # noisy_original = forward(original_image.cpu(), prev_t)
      if t == len(timesteps) - 2:
        noisy_original = forward(original_image.cpu(), t)
      else:
        noisy_original = forward(original_image.cpu(), prev_t)

      noisy_original = noisy_original.to(device).half()
      pred_prev_image = mask * pred_prev_image + (1 - mask) * noisy_original
      image = pred_prev_image

    clean = image.cpu().detach().numpy()

  return clean</code></pre>
                    
                    <h3>Campanile Inpainting</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/2/campanile_inpainted.png" alt="Inpainted result">
                            <div class="image-caption">Campanile Inpainted Result</div>
                        </div>
                    </div>

                    <h3>Boggle Image Inpainting</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/2/boggle_original.png" alt="Original">
                            <div class="image-caption">Original Boggle</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/2/boggle_mask.png" alt="Mask">
                            <div class="image-caption">Inpainting Mask</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/2/inpainted_boggle.png" alt="Inpainted result">
                            <div class="image-caption">Inpainted Result</div>
                        </div>
                    </div>

                    <h3>Mug Image Inpainting</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/2/mug_original.png" alt="Original mug">
                            <div class="image-caption">Original Mug</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/2/mug_mask.png" alt="Mask">
                            <div class="image-caption">Inpainting Mask</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/2/mug_inpainted.png" alt="Inpainted result">
                            <div class="image-caption">Inpainted Result</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Part 1.7.3: Text-Conditional Image-to-Image -->
            <div class="project-content">
                <h2>Part 1.7.3: Text-Conditional Image-to-Image Translation</h2>
                <div class="project-content-body">
                    <p>Now using custom text prompts to guide the projection, rather than just "a high quality photo". This gives us more control over the style and content of the generated images.</p>
                    
                    <h3>Campanile with Custom Prompt: "an oil painting of a sunset"</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/3/sunset_to_campanielle_i_start_1.png" alt="i_start=1">
                            <div class="image-caption">i_start=1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/sunset_to_campanielle_i_start_3.png" alt="i_start=3">
                            <div class="image-caption">i_start=3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/sunset_to_campanielle_i_start_5.png" alt="i_start=5">
                            <div class="image-caption">i_start=5</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/sunset_to_campanielle_i_start_7.png" alt="i_start=7">
                            <div class="image-caption">i_start=7</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/sunset_to_campanielle_i_start_10.png" alt="i_start=10">
                            <div class="image-caption">i_start=10</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/sunset_to_campanielle_i_start_20.png" alt="i_start=20">
                            <div class="image-caption">i_start=20</div>
                        </div>
                    </div>

                    <h3>Photo of my cat with Custom Prompt: "a Formula 1 race car on the track"</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/3/f1_to_boggle_i_start_1.png" alt="i_start=1">
                            <div class="image-caption">i_start=1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/f1_to_boggle_i_start_3.png" alt="i_start=3">
                            <div class="image-caption">i_start=3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/f1_to_boggle_i_start_5.png" alt="i_start=5">
                            <div class="image-caption">i_start=5</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/f1_to_boggle_i_start_7.png" alt="i_start=7">
                            <div class="image-caption">i_start=7</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/f1_to_boggle_i_start_10.png" alt="i_start=10">
                            <div class="image-caption">i_start=10</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/f1_to_boggle_i_start_20.png" alt="i_start=20">
                            <div class="image-caption">i_start=20</div>
                        </div>
                    </div>
                    <h3>Photo of a mug with Custom Prompt: "a CrossFit athlete with blonde hair doing a workout"</h3>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.7/3/crossfit_to_mug_i_start_1.png" alt="i_start=1">
                            <div class="image-caption">i_start=1</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/crossfit_to_mug_i_start_3.png" alt="i_start=3">
                            <div class="image-caption">i_start=3</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/crossfit_to_mug_i_start_5.png" alt="i_start=5">
                            <div class="image-caption">i_start=5</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/crossfit_to_mug_i_start_7.png" alt="i_start=7">
                            <div class="image-caption">i_start=7</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/crossfit_to_mug_i_start_10.png" alt="i_start=10">
                            <div class="image-caption">i_start=10</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.7/3/crossfit_to_mug_i_start_20.png" alt="i_start=20">
                            <div class="image-caption">i_start=20</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Part 1.8: Visual Anagrams -->
            <div class="project-content">
                <h2>Part 1.8: Visual Anagrams</h2>
                <div class="project-content-body">
                    <p>Creating optical illusions where an image shows one thing right-side up and something completely different when flipped upside down!</p>
                    <p><strong>Algorithm:</strong> ε₁ = CFG of UNet(x_t, t, p₁), ε₂ = flip(CFG of UNet(flip(x_t), t, p₂)), ε = (ε₁ + ε₂)/2</p>
                    <pre><code>def make_flip_illusion(image, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  p1 = prompt_embeds[0]
  p2 = prompt_embeds[1]
  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      t = timesteps[i]
      prev_t = timesteps[i+1]
      alpha_t_bar = alphas_cumprod[t]
      alpha_t_prime_bar = alphas_cumprod[prev_t]
      alpha_t = alpha_t_bar / alpha_t_prime_bar
      beta_t = 1 - alpha_t

      cond_output_1 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=p1.half().cuda(),
          return_dict=False
      )[0]
      uncond_output_1 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds.half().cuda(),
          return_dict=False
      )[0]
      noise_est_1, predicted_variance_1 = torch.split(cond_output_1, image.shape[1], dim=1)
      uncond_noise_1, _ = torch.split(uncond_output_1, image.shape[1], dim=1)
      epsilon_1 = uncond_noise_1 + scale * (noise_est_1 - uncond_noise_1)

      image_flipped = torch.flip(image, dims=[2])
      cond_output_2 = stage_1.unet(
          image_flipped,
          t,
          encoder_hidden_states=p2.half().cuda(),
          return_dict=False
      )[0]
      uncond_output_2 = stage_1.unet(
          image_flipped,
          t,
          encoder_hidden_states=uncond_prompt_embeds.half().cuda(),
          return_dict=False
      )[0]
      noise_est_2, predicted_variance_2 = torch.split(cond_output_2, image.shape[1], dim=1)
      uncond_noise_2, _ = torch.split(uncond_output_2, image.shape[1], dim=1)
      epsilon_2 = uncond_noise_2 + scale * (noise_est_2 - uncond_noise_2)

      #now we need to reverse /flip al of the 2 stuff
      epsilon_2 = torch.flip(epsilon_2, dims=[2])
      epsilon = (epsilon_1 + epsilon_2) / 2

      sqrt_alpha_t_bar = torch.sqrt(alpha_t_bar)
      sqrt_1_minus_alpha_t_bar = torch.sqrt(1 - alpha_t_bar)
      x_0 = (image - sqrt_1_minus_alpha_t_bar * epsilon) / sqrt_alpha_t_bar
      sqrt_alpha_t_prime_bar = torch.sqrt(alpha_t_prime_bar)
      sqrt_alpha_t = torch.sqrt(alpha_t)
      term1 = (sqrt_alpha_t_prime_bar * beta_t) / (1 - alpha_t_bar) * x_0
      term2 = (sqrt_alpha_t * (1 - alpha_t_prime_bar)) / (1 - alpha_t_bar) * image
      pred_prev_image = add_variance(predicted_variance_1, t, term1 + term2)
      image = pred_prev_image
  return image.cpu()</code></pre>
                    
                    <h3>Visual Anagram 1: Eiffel tower and golden gate bridge</h3>
                    <p><strong>Prompt 1 (normal):</strong> "the Eiffel Tower"</p>
                    <p><strong>Prompt 2 (upside down):</strong> "the Golden Gate Bridge"</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.8/eiffel_to_bridge_normal.png" alt="Anagram normal">
                            <div class="image-caption">Normal View</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.8/eiffel_to_bridge_normal.png" alt="Anagram flipped" style="transform: rotate(180deg);">
                            <div class="image-caption">Flipped 180° View</div>
                        </div>
                    </div>

                    <h3>Visual Anagram 2: Studying to Gym</h3>
                    <p><strong>Prompt 1 (normal):</strong> "a blonde woman lifting weights at the gym"</p>
                    <p><strong>Prompt 2 (upside down):</strong> "a blonde woman studying at a library desk"</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.8/studying_to_gym.png" alt="Anagram normal">
                            <div class="image-caption">Normal View</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.8/studying_to_gym.png" alt="Anagram flipped" style="transform: rotate(180deg);">
                            <div class="image-caption">Flipped 180° View</div>
                        </div>
                    </div>


                    <h3>Visual Anagram 4: Laptop</h3>
                    <p><strong>Prompt 1 (normal):</strong> "a laptop showing lines of code"</p>
                    <p><strong>Prompt 2 (upside down):</strong> "a French language textbook open on a desk"</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./1.8/laptop.png" alt="Anagram normal">
                            <div class="image-caption">Normal View</div>
                        </div>
                        <div class="image-container">
                            <img src="./1.8/laptop.png" alt="Anagram flipped" style="transform: rotate(180deg);">
                            <div class="image-caption">Flipped 180° View</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Part 1.9: Hybrid Images -->
            <div class="project-content">
                <h2>Part 1.9: Hybrid Images</h2>
                <div class="project-content-body">
                    <p>Implementing Factorized Diffusion to create hybrid images. We combine low frequencies from one noise estimate with high frequencies from another, similar to Project 2!</p>
                    <pre><code>def make_hybrids(image, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  p1 = prompt_embeds[0]
  p2 = prompt_embeds[1]
  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      t = timesteps[i]
      prev_t = timesteps[i+1]
      alpha_t_bar = alphas_cumprod[t]
      alpha_t_prime_bar = alphas_cumprod[prev_t]
      alpha_t = alpha_t_bar / alpha_t_prime_bar
      beta_t = 1 - alpha_t

      cond_output_1 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=p1.half().cuda(),
          return_dict=False
      )[0]
      uncond_output_1 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds.half().cuda(),
          return_dict=False
      )[0]
      noise_est_1, predicted_variance_1 = torch.split(cond_output_1, image.shape[1], dim=1)
      uncond_noise_1, _ = torch.split(uncond_output_1, image.shape[1], dim=1)
      epsilon_1 = uncond_noise_1 + scale * (noise_est_1 - uncond_noise_1)

      cond_output_2 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=p2.half().cuda(),
          return_dict=False
      )[0]
      uncond_output_2 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds.half().cuda(),
          return_dict=False
      )[0]
      noise_est_2, predicted_variance_2 = torch.split(cond_output_2, image.shape[1], dim=1)
      uncond_noise_2, _ = torch.split(uncond_output_2, image.shape[1], dim=1)
      epsilon_2 = uncond_noise_2 + scale * (noise_est_2 - uncond_noise_2)

      #now we need to lowpass/high pass
      epsilon = lowpass(epsilon_1) + highpass(epsilon_2)

      sqrt_alpha_t_bar = torch.sqrt(alpha_t_bar)
      sqrt_1_minus_alpha_t_bar = torch.sqrt(1 - alpha_t_bar)
      x_0 = (image - sqrt_1_minus_alpha_t_bar * epsilon) / sqrt_alpha_t_bar
      sqrt_alpha_t_prime_bar = torch.sqrt(alpha_t_prime_bar)
      sqrt_alpha_t = torch.sqrt(alpha_t)
      term1 = (sqrt_alpha_t_prime_bar * beta_t) / (1 - alpha_t_bar) * x_0
      term2 = (sqrt_alpha_t * (1 - alpha_t_prime_bar)) / (1 - alpha_t_bar) * image
      pred_prev_image = add_variance(predicted_variance_1, t, term1 + term2)
      image = pred_prev_image
  return image.cpu()
</code></pre>
                    <p><strong>Parameters:</strong></p>
                    <ul>
                        <li>Gaussian kernel size: 33</li>
                        <li>Sigma: 2</li>
                    </ul>
                    <p><strong>Formula:</strong> ε = f_lowpass(ε₁) + f_highpass(ε₂)</p>
                    
                    <h3>Hybrid Image 1: Cat & Tiger</h3>
                    <p><strong>Low Frequency Prompt:</strong> 'a grey cat sitting peacefully'</p>
                    <p><strong>High Frequency Prompt:</strong> 'a tiger in the jungle'</p>
                    
                    <div class="image-gallery">
                        <div class="image-container" style="max-width: 600px; margin: 0 auto;">
                            <img src="./1.9/cat_tiger_hybrid.png" alt="Cat Tiger Hybrid">
                            <div class="image-caption">Hybrid Image: Cat (far) & Tiger (near)</div>
                        </div>
                    </div>

                    <h3>Hybrid Image 2: Coffee & Book</h3>
                    <p><strong>Low Frequency Prompt:</strong> 'a coffee cup on a table'</p>
                    <p><strong>High Frequency Prompt:</strong> 'a book open on a desk'</p>
                    
                    <div class="image-gallery">
                        <div class="image-container" style="max-width: 600px; margin: 0 auto;">
                            <img src="./1.9/coffee_book_hybrid.png" alt="Coffee Book Hybrid">
                            <div class="image-caption">Hybrid Image: Coffee (far) & Book (near)</div>
                        </div>
                    </div>

                </div>
            </div>

            <!-- Part B: Flow Matching from Scratch -->
            <div class="project-content">
                <h2>Part B: Flow Matching from Scratch!</h2>
                <div class="project-content-body">
                    <p>In Part B, we train our own flow matching model on MNIST to generate handwritten digits from scratch. This involves building and training a UNet architecture with time and class conditioning.</p>
                </div>
            </div>

            <!-- Part 1: Single-Step Denoising UNet -->
            <div class="project-content">
                <h2>Part 1: Training a Single-Step Denoising UNet</h2>
                <div class="project-content-body">
                    
                    <h3>1.1 Implementing the UNet</h3>
                    <p>[Explain the UNet architecture implementation with downsampling and upsampling blocks]</p>
                    
                    <h3>1.2 Using the UNet to Train a Denoiser</h3>
                    
                    <h4>Visualizing the Noising Process</h4>
                    <p>Visualization of adding different levels of noise (σ) to clean MNIST images:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/1.2/noising_process.png" alt="Noising process">
                            <div class="image-caption">Noising Process with σ = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]</div>
                        </div>
                    </div>

                    <h4>1.2.1 Training</h4>
                    <p><strong>Training Details:</strong></p>
                    <ul>
                        <li>Dataset: MNIST training set</li>
                        <li>Batch size: 256</li>
                        <li>Hidden dimension D: 128</li>
                        <li>Optimizer: Adam (lr=1e-4)</li>
                        <li>Epochs: 5</li>
                        <li>Noise level: σ = 0.5</li>
                    </ul>

                    <h5>Training Loss Curve</h5>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/1.2.1/loss_curve.png" alt="Training loss">
                            <div class="image-caption">Training Loss over Iterations</div>
                        </div>
                    </div>

                    <h5>Denoising Results</h5>
                    <p>Sample results on test set with σ = 0.5:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/1.2.1/results_epoch1.png" alt="Epoch 1 results">
                            <div class="image-caption">Results after Epoch 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./partb/1.2.1/results_epoch5.png" alt="Epoch 5 results">
                            <div class="image-caption">Results after Epoch 5</div>
                        </div>
                    </div>

                    <h4>1.2.2 Out-of-Distribution Testing</h4>
                    <p>Testing the denoiser on different noise levels it wasn't trained on:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/1.2.2/ood_results.png" alt="OOD results">
                            <div class="image-caption">Results with varying σ = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]</div>
                        </div>
                    </div>

                    <p>[Discuss how the denoiser performs on noise levels it wasn't trained on]</p>

                    <h4>1.2.3 Denoising Pure Noise</h4>
                    <p>Training a denoiser on pure Gaussian noise (σ ~ N(0, I)):</p>

                    <h5>Training Loss Curve</h5>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/1.2.3/loss_curve_pure_noise.png" alt="Pure noise training loss">
                            <div class="image-caption">Training Loss for Pure Noise Denoising</div>
                        </div>
                    </div>

                    <h5>Results</h5>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/1.2.3/pure_noise_epoch1.png" alt="Pure noise epoch 1">
                            <div class="image-caption">Results after Epoch 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./partb/1.2.3/pure_noise_epoch5.png" alt="Pure noise epoch 5">
                            <div class="image-caption">Results after Epoch 5</div>
                        </div>
                    </div>

                    <p>[Discuss the patterns observed: The model learns to predict the mean/centroid of all training digits, resulting in blurry averaged digits since MSE loss encourages predicting the point that minimizes squared distances to all training examples.]</p>
                </div>
            </div>

            <!-- Part 2: Flow Matching Model -->
            <div class="project-content">
                <h2>Part 2: Training a Flow Matching Model</h2>
                <div class="project-content-body">
                    
                    <h3>2.1 Adding Time Conditioning to UNet</h3>
                    <p>[Explain how time conditioning is added using FCBlocks to modulate the UNet at different layers]</p>

                    <h3>2.2 Training the Time-Conditioned UNet</h3>
                    <p><strong>Training Details:</strong></p>
                    <ul>
                        <li>Dataset: MNIST training set</li>
                        <li>Batch size: 64</li>
                        <li>Hidden dimension D: 64</li>
                        <li>Optimizer: Adam (initial lr=1e-2)</li>
                        <li>Learning rate scheduler: Exponential decay (γ=0.1^(1/num_epochs))</li>
                        <li>Epochs: [YOUR NUMBER]</li>
                    </ul>

                    <h4>Training Loss Curve</h4>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/2.2/time_cond_loss.png" alt="Time-conditioned training loss">
                            <div class="image-caption">Training Loss for Time-Conditioned UNet</div>
                        </div>
                    </div>

                    <h3>2.3 Sampling from the Time-Conditioned UNet</h3>
                    <p>Iterative denoising results using Algorithm B.2:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/2.3/samples_epoch1.png" alt="Samples epoch 1">
                            <div class="image-caption">Samples after Epoch 1</div>
                        </div>
                        <div class="image-container">
                            <img src="./partb/2.3/samples_epoch5.png" alt="Samples epoch 5">
                            <div class="image-caption">Samples after Epoch 5</div>
                        </div>
                        <div class="image-container">
                            <img src="./partb/2.3/samples_epoch10.png" alt="Samples epoch 10">
                            <div class="image-caption">Samples after Epoch 10</div>
                        </div>
                    </div>

                    <p>[Discuss the quality of generated digits - they should be legible but not perfect]</p>

                    <h3>2.4 Adding Class-Conditioning to UNet</h3>
                    <p>[Explain class-conditioning implementation using one-hot vectors and 10% dropout for classifier-free guidance]</p>

                    <h3>2.5 Training the Class-Conditioned UNet</h3>
                    <p><strong>Training Details:</strong> Same as 2.2 but with class conditioning and dropout (p_uncond = 0.1)</p>

                    <h4>Training Loss Curve</h4>
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/2.5/class_cond_loss.png" alt="Class-conditioned training loss">
                            <div class="image-caption">Training Loss for Class-Conditioned UNet</div>
                        </div>
                    </div>

                    <h3>2.6 Sampling from the Class-Conditioned UNet</h3>
                    <p>Using classifier-free guidance with γ = 5.0 to generate digits:</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/2.6/class_samples_epoch1.png" alt="Class samples epoch 1">
                            <div class="image-caption">Samples after Epoch 1 (4 instances of each digit 0-9)</div>
                        </div>
                        <div class="image-container">
                            <img src="./partb/2.6/class_samples_epoch5.png" alt="Class samples epoch 5">
                            <div class="image-caption">Samples after Epoch 5</div>
                        </div>
                        <div class="image-container">
                            <img src="./partb/2.6/class_samples_epoch10.png" alt="Class samples epoch 10">
                            <div class="image-caption">Samples after Epoch 10</div>
                        </div>
                    </div>

                    <h4>Removing the Learning Rate Scheduler</h4>
                    <p>[Describe what you did to compensate for removing the exponential learning rate scheduler]</p>
                    
                    <div class="image-gallery">
                        <div class="image-container">
                            <img src="./partb/2.6/no_scheduler_results.png" alt="Results without scheduler">
                            <div class="image-caption">Results without Learning Rate Scheduler</div>
                        </div>
                    </div>

                    <p>[Explain your approach and whether performance was maintained]</p>
                </div>
            </div>

            <!-- Part 3: Bells & Whistles (Optional) -->
            <div class="project-content">
                <h2>Part 3: Bells & Whistles (Optional)</h2>
                <div class="project-content-body">
                    <p>[Include any additional explorations:</p>
                    <ul>
                        <li>Better time-conditioned only UNet (CS280A required)</li>
                        <li>Training on SVHN, Fashion-MNIST, or CIFAR10</li>
                        <li>Your own creative ideas</li>
                    </ul>
                    <p>]</p>
                </div>
            </div>

            <!-- Reflections for Entire Project 5 -->
            <div class="project-content">
                <h2>Project 5 Reflections and Learnings</h2>
                <div class="project-content-body">
                    <p>[Write about what you learned across both parts A and B:</p>
                    <ul>
                        <li>Understanding of diffusion models and flow matching</li>
                        <li>Differences between one-step and iterative denoising</li>
                        <li>The importance of conditioning (time and class)</li>
                        <li>Implementation challenges and solutions</li>
                        <li>Most interesting results from both parts</li>
                        <li>Ideas for future exploration</li>
                    </ul>
                    <p>]</p>
                </div>
            </div>

        </main>

        <footer>
            <p>&copy; 2025 CS180 Portfolio • Sylvie Venuto • UC Berkeley</p>
        </footer>
    </div>
</body>
</html>
